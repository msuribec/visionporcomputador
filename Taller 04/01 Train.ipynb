{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Taller 04: Tareas de Pretexto\n",
    "\n",
    "* María Sofía Uribe\n",
    "* Javier Daza Olivella\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar un modelo CNN o ViT.\n",
    "\n",
    "• Puede ser un modelo de alguna librería (Keras) o una implementación “vanilla” de una CNN\n",
    "\n",
    "• Inventar una tarea de pretexto\n",
    "\n",
    "• Entrenar en ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Definimos La tarea de pretexto: Rotacion\n",
    "4 Angulos, 0°, 90°, 180° y 270°\n",
    "\n",
    "\n",
    "Construimos Clase Dataset, que sera la encargada de gestionar el __getitem__ que sera luego usado en el dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RotationDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.dataset = datasets.ImageFolder(root=root)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * 4  # 4 rotaciones por imagen\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        rot_class = idx % 4\n",
    "        img, _ = self.dataset[img_idx]\n",
    "\n",
    "        angle = [0, 90, 180, 270][rot_class]\n",
    "        img = img.rotate(angle)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, rot_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el compose buscamos transformar cada imagen que entra al dataloader:\n",
    "\n",
    "- Hacemos un Resize\n",
    "\n",
    "- Luego hacemos un crop al centro de la imagen \n",
    "\n",
    "- Luego convertimos a tensor\n",
    "\n",
    "- Finalmente Normalizamos usando los valores del dataset original de tiny Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "train_dir = \"./data/tiny-imagenet-200/train\" \n",
    "dataset = RotationDataset(train_dir, transform)\n",
    "\n",
    "# Construimos nuestro dataloader que tendra un batch_size de 128\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el modelo resnet18\n",
    "\n",
    "Descongelar ultimas 2 capas layer3 y layer4 y agregamos una nueva capa lineal, que nos dara la prediccion final, las rotaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Congelar todo el modelo\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Descongelar ultimas 2 capas layer3 y layer4 y agregamos una nueva capa lineal, que nos dara la prediccion final, las rotaciones\n",
    "for param in model.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 4)  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos nuestro criterion, que sera el encargado de obtener la loss y donde se calculan los gradientes\n",
    "\n",
    "y seleccionamos el optimizer Adam, que ajustara la direcion y los pasos basado en el gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 3125/3125 [02:13<00:00, 23.43batch/s, Batch Loss=0.6643, Running Loss=0.7818, Running Acc=68.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed: Loss=0.7818, Acc=68.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 3125/3125 [02:37<00:00, 19.90batch/s, Batch Loss=0.5236, Running Loss=0.5988, Running Acc=76.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed: Loss=0.5988, Acc=76.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3125/3125 [02:32<00:00, 20.48batch/s, Batch Loss=0.3857, Running Loss=0.4639, Running Acc=81.99%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed: Loss=0.4639, Acc=81.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 3125/3125 [02:37<00:00, 19.88batch/s, Batch Loss=0.2470, Running Loss=0.3235, Running Acc=87.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed: Loss=0.3235, Acc=87.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 3125/3125 [02:14<00:00, 23.21batch/s, Batch Loss=0.1997, Running Loss=0.2136, Running Acc=91.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed: Loss=0.2136, Acc=91.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 3125/3125 [01:40<00:00, 31.08batch/s, Batch Loss=0.1441, Running Loss=0.1498, Running Acc=94.44%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed: Loss=0.1498, Acc=94.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 3125/3125 [01:40<00:00, 31.05batch/s, Batch Loss=0.1127, Running Loss=0.1179, Running Acc=95.67%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 completed: Loss=0.1179, Acc=95.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 3125/3125 [01:53<00:00, 27.64batch/s, Batch Loss=0.1064, Running Loss=0.0983, Running Acc=96.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed: Loss=0.0983, Acc=96.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 3125/3125 [01:40<00:00, 31.16batch/s, Batch Loss=0.1027, Running Loss=0.0874, Running Acc=96.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 completed: Loss=0.0874, Acc=96.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 3125/3125 [01:40<00:00, 31.11batch/s, Batch Loss=0.0657, Running Loss=0.0783, Running Acc=97.19%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed: Loss=0.0783, Acc=97.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4) \n",
    "\n",
    "# Iniciamos Entrenamiento\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\")\n",
    "    for imgs, labels in progress_bar:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * imgs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        running_loss = total_loss / total\n",
    "        running_acc = 100 * correct / total\n",
    "        progress_bar.set_postfix({\"Batch Loss\": f\"{batch_loss:.4f}\", \"Running Loss\": f\"{running_loss:.4f}\", \"Running Acc\": f\"{running_acc:.2f}%\"})\n",
    "    \n",
    "    epoch_loss = total_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1} completed: Loss={epoch_loss:.4f}, Acc={epoch_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "\n",
    "- Logramos un accuracy de 97%\n",
    "- Usamos un backbone bastante simple: Resnet18 \n",
    "- 10 Epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"resnet18_rotation_pretext.pth\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
